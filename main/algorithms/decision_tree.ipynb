{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is a measure of how `disordered` a collection is.  \n",
    "The `more` impure the feature is, the higher the entropy.  \n",
    "\n",
    "Probability distribution is the `frequency` of the unique values.  \n",
    "It turns out that a `logarithm` of the number of states is perfect for disorder.  \n",
    "\n",
    "$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "A = ['apple' 'orange' 'orange' 'banana' 'banana']\n",
      "B = ['apple' 'apple' 'apple' 'apple' 'apple' 'orange' 'orange']\n",
      "\n",
      " Probability distributions (by hand):\n",
      "[0.2, 0.4, 0.4]\n",
      "[0.7142857142857143, 0.2857142857142857, 0.0]\n",
      "\n",
      " Probability distributions (with pandas):\n",
      "[0.4 0.4 0.2]\n",
      "[0.71428571 0.28571429]\n",
      "\n",
      " Entropies:\n",
      "1.5219280948873621\n",
      "0.863120568566631\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Tree / Entropy\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the initial traning data\n",
    "A = ['apple']*1 + ['orange']*2 + ['banana']*2\n",
    "B = ['apple']*5 + ['orange']*2 + ['banana']*0\n",
    "\n",
    "# Probability (by hand)\n",
    "P1 = [1/5, 2/5, 2/5] \n",
    "P2 = [5/7, 2/7, 0/7]\n",
    "\n",
    "# Probability (with pandas)\n",
    "A = pd.Series(A)\n",
    "B = pd.Series(B)\n",
    "\n",
    "P3 = A.value_counts(normalize=True)\n",
    "P4 = B.value_counts(normalize=True)\n",
    "\n",
    "# Entropy (Shannon model)\n",
    "H3 = -1 * np.sum(P3 * np.log2(P3))\n",
    "H4 = -1 * np.sum(P4 * np.log2(P4))\n",
    "assert H3 > H4\n",
    "\n",
    "# Output results\n",
    "print(\"Datasets:\")\n",
    "print(\"A =\", A.values)\n",
    "print(\"B =\", B.values)\n",
    "\n",
    "print(\"\\n Probability distributions (by hand):\")\n",
    "print(P1)\n",
    "print(P2)\n",
    "\n",
    "print(\"\\n Probability distributions (with pandas):\")\n",
    "print(P3.values)\n",
    "print(P4.values)\n",
    "\n",
    "print(\"\\n Entropies:\")\n",
    "print(H3)\n",
    "print(H4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "\n",
    "Information gain is a measure of the `reduction` in entropy.  \n",
    "As the entropy of an attribute `increases`, the information gain decreases.  \n",
    "We can find the most useful attribute and `split` the dataset based on that attribute.  \n",
    "\n",
    "$ \n",
    "    \\text{IG}(X, A) = \\text{H}(X) - \\sum_{v \\in \\text{Values}(A)} \\frac{|X_v|}{|X|}  \\cdot \\text{H}(X_v) \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset:\n",
      "     outlook  temp humidity  windy play\n",
      "0      sunny   hot     high  False   no\n",
      "1      sunny   hot     high   True   no\n",
      "2   overcast   hot     high  False  yes\n",
      "3      rainy  mild     high  False  yes\n",
      "4      rainy  cool   normal  False  yes\n",
      "5      rainy  cool   normal   True   no\n",
      "6   overcast  cool   normal   True  yes\n",
      "7      sunny  mild     high  False   no\n",
      "8      sunny  cool   normal  False  yes\n",
      "9      rainy  mild   normal  False  yes\n",
      "10     sunny  mild   normal   True  yes\n",
      "11  overcast  mild     high   True  yes\n",
      "12  overcast   hot   normal  False  yes\n",
      "13     rainy  mild     high   True   no\n",
      "\n",
      " Describe:\n",
      "       outlook  temp humidity  windy play\n",
      "count       14    14       14     14   14\n",
      "unique       3     3        2      2    2\n",
      "top      sunny  mild     high  False  yes\n",
      "freq         5     6        7      8    9\n",
      "\n",
      " Entropy:\n",
      "0.9402859586706311\n",
      "\n",
      " AttrEntropy:\n",
      "{'outlook': 0.6935361388961914, 'temp': 0.9110633930116756, 'humidity': 0.7884504573082889, 'windy': 0.892158928262361}\n",
      "\n",
      " Information gains:\n",
      "{'outlook': 0.24674981977443977, 'temp': 0.029222565658955535, 'humidity': 0.15183550136234225, 'windy': 0.048127030408270155}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Trees / Info Gain\n",
    "Target: Play Tennis or not\n",
    "\n",
    "Information gain example (for feature 'wind'):\n",
    "IG = H - (8/14)H_weak - (6/14)H_strong\n",
    "IG = 0.940 - (8/14)0.811 - (6/14)1.00 = 0.048 \n",
    "\n",
    "Machine epsilon is the upper bound on the relative error due to rounding.\n",
    "This small value added to the denominator in order to avoid division by zero. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('decision_tree/data/play_tennis.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y), play tennis yes/no\n",
    "X = df.drop(['play'], axis=1)\n",
    "y = df['play'] \n",
    "\n",
    "\"\"\" Calculate the total entropy of the dataset \n",
    "\"\"\"\n",
    "total_entropy = 0\n",
    "\n",
    "# For each unique play outcome (yes/no)\n",
    "for target in df['play'].unique():\n",
    "\n",
    "    target_count = df['play'].value_counts()[target]\n",
    "    target_total = len(df['play'])\n",
    "\n",
    "    P = target_count/target_total # probability of target outcome\n",
    "    total_entropy += -P*np.log2(P) # total entropy using the probability\n",
    "\n",
    "\"\"\" Function to calculate entropy of a specific attribute\n",
    "\"\"\"\n",
    "def attribute_entropy(attr):\n",
    "    E = 0\n",
    "    \n",
    "    eps = np.finfo(float).eps  # Calculate machine epsilon for float operations\n",
    "\n",
    "    targets = df.play.unique() # Get unique play outcomes (yes/no)\n",
    "    values = df[attr].unique() # Get unique values for the given attribute\n",
    "\n",
    "    # For each unique value of the attribute (cool/hot)\n",
    "    for v in values:\n",
    "\n",
    "        # Initialize entropy for this value\n",
    "        ent = 0\n",
    "\n",
    "        # For each unique play outcome (yes/no)\n",
    "        for t in targets:\n",
    "\n",
    "            # Count occurrences where attribute=value and play outcome matches\n",
    "            numerator = len(df[attr][df[attr] == v][df.play == t]) # numerator\n",
    "\n",
    "            # Count occurrences where attribute=value\n",
    "            denominator = len(df[attr][df[attr] == v])\n",
    "\n",
    "            # Calculate a fraction related to the probability\n",
    "            fraction = numerator/(denominator + eps)\n",
    "\n",
    "            # Update entropy for this value\n",
    "            ent += -fraction*np.log2(fraction + eps)\n",
    "\n",
    "        # Update total entropy using the weighted entropy for this value\n",
    "        E += -(denominator/len(df))*ent # sum of all entropies\n",
    "\n",
    "    # Return the absolute value of the entropy\n",
    "    return abs(E)\n",
    "\n",
    "\n",
    "# Get the names of attributes (excluding the target variable)\n",
    "attributes = df.keys()[:-1]\n",
    "\n",
    "# Calculate entropy for each attribute and store it in a dictionary\n",
    "E = {} \n",
    "for k in attributes:\n",
    "    E[k] = attribute_entropy(k)\n",
    "\n",
    "# Calculate information gain for each attribute and store it in a dictionary\n",
    "IG = {}\n",
    "for k in E:\n",
    "    IG[k] = total_entropy - E[k]\n",
    "\n",
    "# Alternatives one-liner versions to calculate entropy and information gain\n",
    "E  = {k:attribute_entropy(k) for k in attributes}\n",
    "IG = {k:(total_entropy - E[k]) for k in E} \n",
    "\n",
    "# Asserts\n",
    "assert E['outlook']  < E['humidity']\n",
    "assert IG['outlook'] > IG['humidity'] # Look Here\n",
    "\n",
    "# Output results\n",
    "print(\"\\n Dataset:\"); print(df)\n",
    "print(\"\\n Describe:\"); print(df.describe())\n",
    "print(\"\\n Entropy:\"); print(total_entropy)\n",
    "print(\"\\n AttrEntropy:\"); print(E)\n",
    "print(\"\\n Information gains:\"); print(IG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm\n",
    "\n",
    "We build a decision `tree` recursively giving priority to the attributes with the higher IG.  \n",
    "Iterative Dichotomiser 3 is a `classification` algorithm.  \n",
    "\n",
    "It follows a `greedy` approach of building a decision tree.  \n",
    "It gives `priority` to the attributes with the higher information gain.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Decistion Tree:\n",
      "{'outlook': {'overcast': 'yes', 'rainy': {'temp': {'cool': {'humidity': {'normal': {'windy': {False: 'yes', True: 'no'}}}}, 'mild': {'humidity': {'high': {'windy': {False: 'yes', True: 'no'}}, 'normal': 'yes'}}}}, 'sunny': {'temp': {'cool': 'yes', 'hot': 'no', 'mild': {'humidity': {'high': 'no', 'normal': 'yes'}}}}}} \n",
      "\n",
      " outlook = overcast : yes\n",
      " outlook = rainy :\n",
      "  temp = cool :\n",
      "   humidity = normal :\n",
      "    windy = False : yes\n",
      "    windy = True : no\n",
      "  temp = mild :\n",
      "   humidity = high :\n",
      "    windy = False : yes\n",
      "    windy = True : no\n",
      "   humidity = normal : yes\n",
      " outlook = sunny :\n",
      "  temp = cool : yes\n",
      "  temp = hot : no\n",
      "  temp = mild :\n",
      "   humidity = high : no\n",
      "   humidity = normal : yes\n",
      "\n",
      " Example usage 1:\n",
      "Attributes: {'outlook': 'sunny', 'temp': 'mild', 'humidity': 'high', 'windy': False}\n",
      "Prediction: no\n",
      "\n",
      " Example usage 2:\n",
      "Attributes: {'outlook': 'rainy', 'temp': 'mild', 'humidity': 'normal', 'windy': True}\n",
      "Prediction: yes\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Trees / ID3 Algorithm\n",
    "\n",
    "1. Calculate entropy for dataset\n",
    "2. For each attribute:\n",
    "   Calculate entropy for all categorical values\n",
    "   Calculate information gain for the current attribute\n",
    "3. Find the feture with maximum information gain\n",
    "4. Repeat\n",
    "\n",
    "For example, the `outlook` has the highest info gain (0.24).\n",
    "It we will select it as the root node for the start level of splitting.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn import tree\n",
    "\n",
    "# Dataset\n",
    "df = pd.read_csv('decision_tree/data/play_tennis.csv')\n",
    "\n",
    "# Training data\n",
    "X = df.drop(['play'], axis=1)\n",
    "y = df['play']\n",
    "\n",
    "\n",
    "# Total entropy (for current dataframe)\n",
    "def dataset_entropy(df):\n",
    "    E = 0\n",
    "    N = df['play'].value_counts() # yes: 9, no: 5\n",
    "    values = df['play'].unique()\n",
    "    for v in values: # yes/no\n",
    "        P = N[v]/len(df['play'])  # probability\n",
    "        E += -P*np.log2(P)\n",
    "    return E\n",
    "\n",
    "# Entropy for each attribute\n",
    "def attribute_entropy(df, attr):\n",
    "    E = 0\n",
    "    eps = np.finfo(float).eps   # machine epsilon for the float \n",
    "    targets = df.play.unique()\n",
    "    values = df[attr].unique()\n",
    "    for v in values: # cool/hot\n",
    "        ent = 0\n",
    "        for t in targets: # yes,no\n",
    "            num = len(df[attr][df[attr] == v][df.play == t]) # numerator\n",
    "            den = len(df[attr][df[attr] == v])\n",
    "            fraction = num/(den + eps)\n",
    "            ent += -fraction*np.log2(fraction + eps) # entropy for one feature\n",
    "        E += -(den/len(df))*ent # sum of all entropies\n",
    "    return abs(E)\n",
    "\n",
    "# Find attribute with maximum information gain\n",
    "def find_winner(df):\n",
    "    IG = {}\n",
    "    attributes = df.keys()[:-1]\n",
    "\n",
    "    # Loop for attributes in dataframe and compute info gains\n",
    "    for attr in attributes: \n",
    "        IG[attr] = dataset_entropy(df) - attribute_entropy(df, attr)\n",
    "    winner = attributes[np.argmax(IG)] # maxim info gains\n",
    "    return winner\n",
    "\n",
    "\n",
    "# Construct the decision tree (dictionary)\n",
    "def buildTree(df):\n",
    "    tree = {}\n",
    "\n",
    "    # Target column\n",
    "    Class = df.keys()[-1] # play\n",
    "    \n",
    "    # Maximum info gain\n",
    "    node = find_winner(df) # outlook\n",
    "    tree[node] = {}\n",
    "\n",
    "    # Distinct values\n",
    "    values = np.unique(df[node]) # overcast/rain\n",
    "\n",
    "    # Loop throw the attribute values\n",
    "    for value in values:\n",
    "        subtable = df[df[node] == value].reset_index(drop=True)\n",
    "        attr_values, counts = np.unique(subtable[Class], return_counts=True)\n",
    "\n",
    "        if len(counts) == 1: # pure subset\n",
    "            tree[node][value] = attr_values[0]\n",
    "        else:\n",
    "            subtable = subtable.drop(node, axis=1)\n",
    "            tree[node][value] = buildTree(subtable) # Recursive case\n",
    "            \n",
    "    return tree\n",
    "\n",
    "decision_tree = buildTree(df)\n",
    "\n",
    "\n",
    "# Print dictionary tree (recursion  in case of subtrees)\n",
    "def print_tree(tree, attr=None, i=0):\n",
    "    if not attr:\n",
    "        attr = next(iter(tree)) # attrribute in the current tree node\n",
    "\n",
    "    for key, subval in tree[attr].items():\n",
    "\n",
    "        if isinstance(subval, str): # Base case\n",
    "            print(i*\" \", attr, \"=\", key, \":\", subval)\n",
    "            continue\n",
    "        \n",
    "        print(i*\" \", attr, \"=\", key, \":\")\n",
    "        print_tree(subval, i=i+1) # Recursive\n",
    "\n",
    "    return\n",
    "\n",
    "# Predict unknow (only for cases included in the train dataset)\n",
    "def predict(X, tree):\n",
    "    key = next(iter(tree))\n",
    "    val = X[key]\n",
    "    subval = tree[key][val]\n",
    "\n",
    "    if isinstance(subval, str): # Base case\n",
    "        return subval\n",
    "        \n",
    "    subval = predict(X, subval) # Recursive\n",
    "    return subval\n",
    "\n",
    "\n",
    "print(\"\\n Decistion Tree:\")\n",
    "print(decision_tree, \"\\n\")\n",
    "print_tree(decision_tree)\n",
    "\n",
    "print(\"\\n Example usage 1:\")\n",
    "x = {'outlook': 'sunny', 'temp': 'mild', 'humidity': 'high', 'windy': False}\n",
    "y = predict(x, decision_tree)\n",
    "print(\"Attributes:\", x)\n",
    "print(\"Prediction:\", y)\n",
    "\n",
    "print(\"\\n Example usage 2:\")\n",
    "x = {'outlook': 'rainy', 'temp': 'mild', 'humidity': 'normal', 'windy': True}\n",
    "y = predict(x, decision_tree)\n",
    "print(\"Attributes:\", x)\n",
    "print(\"Prediction:\", y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
