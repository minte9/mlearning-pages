{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "In the context of machine learning and `natural` language processing, Count Vectorizer\" refers  \n",
    "to the overall method of `converting` a collection of text documents into a matrix of token counts. \n",
    "\n",
    "This process typically `involves` both the \"fit\" and \"transform\" steps,  \n",
    "which are `encapsulated` in the fit_transform method in scikit-learn's CountVectorizer.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Learn the `vocabulary` (unique words) from the provided text data.   \n",
    "We create a list (set) of all `unique` tokens across all documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Paris', 'London'}\n"
     ]
    }
   ],
   "source": [
    "# Sample text strings\n",
    "a = 'London Paris London'\n",
    "b = 'Paris Paris London'\n",
    "\n",
    "def create_vocabulary(texts):\n",
    "    vocabulary = set()\n",
    "\n",
    "    for t in texts:\n",
    "        # Split each document into tokens (usually words)\n",
    "        words = t.split()\n",
    "\n",
    "        # Create a list of all unique tokens across all documents\n",
    "        for w in words:\n",
    "            vocabulary.add(w)\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = create_vocabulary([a, b])\n",
    "print(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token counts\n",
    "\n",
    "Convert the text documents into a `numerical` format (specifically, a token count matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "def fit_transform(texts):\n",
    "    vocabulary = create_vocabulary(texts) # set\n",
    "    vocabulary = list(vocabulary)\n",
    "    matrix = []\n",
    "\n",
    "    for t in texts:\n",
    "        count_vector = [0] * len(vocabulary)\n",
    "\n",
    "        for word in t.split():\n",
    "            index = vocabulary.index(word)\n",
    "            count_vector[index] += 1\n",
    "\n",
    "        matrix.append(count_vector)\n",
    "    return matrix\n",
    "\n",
    "# Sample text strings\n",
    "a = 'London Paris London'\n",
    "b = 'Paris Paris London'\n",
    "\n",
    "# Get the frequency matrix\n",
    "matrix = fit_transform([a, b])\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
