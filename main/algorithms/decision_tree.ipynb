{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is a measure of how `disordered` a collection is.  \n",
    "The `more` impure the feature is, the higher the entropy.  \n",
    "\n",
    "Probability distribution is the `frequency` of the unique values.  \n",
    "It turns out that a `logarithm` of the number of states is perfect for disorder.  \n",
    "\n",
    "$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "A = ['apple' 'orange' 'orange' 'banana' 'banana']\n",
      "B = ['apple' 'apple' 'apple' 'apple' 'apple' 'orange' 'orange']\n",
      "\n",
      " Probability distributions (by hand):\n",
      "[0.2, 0.4, 0.4]\n",
      "[0.7142857142857143, 0.2857142857142857, 0.0]\n",
      "\n",
      " Probability distributions (with pandas):\n",
      "[0.4 0.4 0.2]\n",
      "[0.71428571 0.28571429]\n",
      "\n",
      " Entropies:\n",
      "1.5219280948873621\n",
      "0.863120568566631\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Tree / Entropy\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the initial traning data\n",
    "A = ['apple']*1 + ['orange']*2 + ['banana']*2\n",
    "B = ['apple']*5 + ['orange']*2 + ['banana']*0\n",
    "\n",
    "# Probability (by hand)\n",
    "P1 = [1/5, 2/5, 2/5] \n",
    "P2 = [5/7, 2/7, 0/7]\n",
    "\n",
    "# Probability (with pandas)\n",
    "A = pd.Series(A)\n",
    "B = pd.Series(B)\n",
    "\n",
    "P3 = A.value_counts(normalize=True)\n",
    "P4 = B.value_counts(normalize=True)\n",
    "\n",
    "# Entropy (Shannon model)\n",
    "H3 = -1 * np.sum(P3 * np.log2(P3))\n",
    "H4 = -1 * np.sum(P4 * np.log2(P4))\n",
    "assert H3 > H4\n",
    "\n",
    "# Output results\n",
    "print(\"Datasets:\")\n",
    "print(\"A =\", A.values)\n",
    "print(\"B =\", B.values)\n",
    "\n",
    "print(\"\\n Probability distributions (by hand):\")\n",
    "print(P1)\n",
    "print(P2)\n",
    "\n",
    "print(\"\\n Probability distributions (with pandas):\")\n",
    "print(P3.values)\n",
    "print(P4.values)\n",
    "\n",
    "print(\"\\n Entropies:\")\n",
    "print(H3)\n",
    "print(H4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini index\n",
    "\n",
    "Both entropy and Gini index can be used as impurity `measures` for decision tree algorithms.  \n",
    "Gini index is `between` 0 and 1, it easy to compare gini across different features.  \n",
    "\n",
    "Gini index is often preferred due to its computational `efficiency`.  \n",
    "Entropy may be more `sensitive` to changes in class probabilities.  \n",
    "\n",
    "$ \\text{Gini}(X) = 1 - \\sum_{x \\in X} P(x)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "A = ['apple' 'orange' 'orange' 'banana' 'banana']\n",
      "B = ['apple' 'apple' 'apple' 'apple' 'apple' 'orange' 'orange']\n",
      "\n",
      " Probability distributions (by hand):\n",
      "orange    0.4\n",
      "banana    0.4\n",
      "apple     0.2\n",
      "dtype: float64\n",
      "apple     0.714286\n",
      "orange    0.285714\n",
      "dtype: float64\n",
      "\n",
      " Gini indexes:\n",
      "0.6399999999999999\n",
      "0.40816326530612246\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Tree / Gini Index\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the initial traning data\n",
    "A = ['apple']*1 + ['orange']*2 + ['banana']*2\n",
    "B = ['apple']*5 + ['orange']*2 + ['banana']*0\n",
    "\n",
    "A = pd.Series(A)\n",
    "B = pd.Series(B)\n",
    "\n",
    "# Probability distribution\n",
    "P1 = A.value_counts(normalize=True)\n",
    "P2 = B.value_counts(normalize=True)\n",
    "\n",
    "# Gini Index\n",
    "g1 = 1 - np.sum(np.square(P1)) # Look Here\n",
    "g2 = 1 - np.sum(np.square(P2))\n",
    "assert g1 > g2\n",
    "\n",
    "# Output results\n",
    "print(\"Datasets:\")\n",
    "print(\"A =\", A.values)\n",
    "print(\"B =\", B.values)\n",
    "\n",
    "print(\"\\n Probability distributions (by hand):\")\n",
    "print(P1)\n",
    "print(P2)\n",
    "\n",
    "print(\"\\n Gini indexes:\")\n",
    "print(g1)\n",
    "print(g2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "\n",
    "Information gain is a measure of the `reduction` in entropy.  \n",
    "As the entropy of an attribute `increases`, the information gain decreases.  \n",
    "We can find the most useful attribute and `split` the dataset based on that attribute.  \n",
    "\n",
    "$ \n",
    "    \\text{IG}(X, A) = \\text{H}(X) - \\sum_{v \\in \\text{Values}(A)} \\frac{|X_v|}{|X|}  \\cdot \\text{H}(X_v) \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset:\n",
      "     outlook  temp humidity  windy play\n",
      "0      sunny   hot     high  False   no\n",
      "1      sunny   hot     high   True   no\n",
      "2   overcast   hot     high  False  yes\n",
      "3      rainy  mild     high  False  yes\n",
      "4      rainy  cool   normal  False  yes\n",
      "5      rainy  cool   normal   True   no\n",
      "6   overcast  cool   normal   True  yes\n",
      "7      sunny  mild     high  False   no\n",
      "8      sunny  cool   normal  False  yes\n",
      "9      rainy  mild   normal  False  yes\n",
      "10     sunny  mild   normal   True  yes\n",
      "11  overcast  mild     high   True  yes\n",
      "12  overcast   hot   normal  False  yes\n",
      "13     rainy  mild     high   True   no\n",
      "\n",
      " Describe:\n",
      "       outlook  temp humidity  windy play\n",
      "count       14    14       14     14   14\n",
      "unique       3     3        2      2    2\n",
      "top      sunny  mild     high  False  yes\n",
      "freq         5     6        7      8    9\n",
      "\n",
      " Entropy:\n",
      "0.9402859586706311\n",
      "\n",
      " AttrEntropy:\n",
      "{'outlook': 0.6935361388961914, 'temp': 0.9110633930116756, 'humidity': 0.7884504573082889, 'windy': 0.892158928262361}\n",
      "\n",
      " Information gains:\n",
      "{'outlook': 0.24674981977443977, 'temp': 0.029222565658955535, 'humidity': 0.15183550136234225, 'windy': 0.048127030408270155}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decision Trees / Info Gain\n",
    "Play Tennis\n",
    "\n",
    "Play Tennis example (information gain for wind):\n",
    "IG = H - (8/14)H_weak - (6/14)H_strong\n",
    "IG = 0.940 - (8/14)0.811 - (6/14)1.00 = 0.048 \n",
    "\n",
    "Machine epsilon is the upper bound on the relative error due to rounding.\n",
    "This small value added to the denominator in order to avoid division by zero. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('decision_tree/data/play_tennis.csv')\n",
    "\n",
    "# Split the dataset into features (X) and the target (y), whether play tennis or not\n",
    "X = df.drop(['play'], axis=1)\n",
    "y = df['play'] \n",
    "\n",
    "# Function to calculate the total entropy of the dataset\n",
    "def dataset_entropy():\n",
    "    E = 0\n",
    "\n",
    "    # Count the occurrences of each play outcome (yes=9, no=5)\n",
    "    N = df['play'].value_counts()\n",
    "\n",
    "    # For each unique play outcome (yes/no)\n",
    "    for v in df['play'].unique():\n",
    "\n",
    "        # Calculate the probability of this outcome\n",
    "        P = N[v]/len(df['play'])\n",
    "\n",
    "         # Update total entropy using the probability\n",
    "        E += -P*np.log2(P)\n",
    "    return E\n",
    "\n",
    "# Function to calculate the entropy of a specific attribute\n",
    "def attribute_entropy(attr):\n",
    "    E = 0\n",
    "\n",
    "    # Calculate machine epsilon for float operations\n",
    "    eps = np.finfo(float).eps \n",
    "\n",
    "    # Get unique play outcomes (yes/no)\n",
    "    targets = df.play.unique()\n",
    "\n",
    "    # Get unique values for the given attribute\n",
    "    values = df[attr].unique()\n",
    "\n",
    "    # For each unique value of the attribute (cool/hot)\n",
    "    for v in values:\n",
    "\n",
    "        # Initialize entropy for this value\n",
    "        ent = 0\n",
    "\n",
    "        # For each unique play outcome (yes/no)\n",
    "        for t in targets:\n",
    "\n",
    "            # Count occurrences where attribute=value and play outcome matches\n",
    "            num = len(df[attr][df[attr] == v][df.play == t]) # numerator\n",
    "\n",
    "            # Count occurrences where attribute=value\n",
    "            den = len(df[attr][df[attr] == v])\n",
    "\n",
    "            # Calculate a fraction related to the probability\n",
    "            fraction = num/(den + eps)\n",
    "\n",
    "            # Update entropy for this value\n",
    "            ent += -fraction*np.log2(fraction + eps)\n",
    "\n",
    "        # Update total entropy using the weighted entropy for this value\n",
    "        E += -(den/len(df))*ent # sum of all entropies\n",
    "\n",
    "    # Return the absolute value of the entropy\n",
    "    return abs(E)\n",
    "\n",
    "\n",
    "# Get the names of attributes (excluding the target variable)\n",
    "attributes = df.keys()[:-1]\n",
    "\n",
    "# Calculate entropy for each attribute and store it in a dictionary\n",
    "E = {} \n",
    "for k in attributes:\n",
    "    E[k] = attribute_entropy(k)\n",
    "\n",
    "# Calculate information gain for each attribute and store it in a dictionary\n",
    "IG = {}\n",
    "for k in E:\n",
    "    IG[k] = dataset_entropy() - E[k]\n",
    "\n",
    "# Alternatives one-liner versions to calculate entropy and information gain\n",
    "E  = {k:attribute_entropy(k) for k in attributes}\n",
    "IG = {k:(dataset_entropy() - E[k]) for k in E} \n",
    "\n",
    "# Asserts\n",
    "assert E['outlook']  < E['humidity']\n",
    "assert IG['outlook'] > IG['humidity'] # Look Here\n",
    "\n",
    "# Output results\n",
    "print(\"\\n Dataset:\"); print(df)\n",
    "print(\"\\n Describe:\"); print(df.describe())\n",
    "print(\"\\n Entropy:\"); print(dataset_entropy())\n",
    "print(\"\\n AttrEntropy:\"); print(E)\n",
    "print(\"\\n Information gains:\"); print(IG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
